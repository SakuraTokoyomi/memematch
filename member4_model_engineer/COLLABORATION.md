# ğŸ¤ åä½œæ¥å£æ–‡æ¡£

æœ¬æ–‡æ¡£è¯´æ˜æˆå‘˜4ï¼ˆæ¨¡å‹å·¥ç¨‹å¸ˆï¼‰ä¸å…¶ä»–æˆå‘˜çš„åä½œå…³ç³»å’Œæ•°æ®æ¥å£ã€‚

---

## ğŸ“¥ è¾“å…¥ä¾èµ–ï¼ˆä»å…¶ä»–æˆå‘˜è·å–ï¼‰

### ä»æˆå‘˜2ï¼ˆæ•°æ®æ„å»ºå·¥ç¨‹å¸ˆï¼‰è·å–

#### 1. è¡¨æƒ…åŒ…åº“ - `data/memes.json`

**é¢„æœŸäº¤ä»˜æ—¶é—´**ï¼šDay 3-4  
**æ ¼å¼**ï¼šJSONæ•°ç»„

```json
[
  {
    "id": "meme_001",
    "label": "é»‘äººé—®å·",
    "keywords": ["å›°æƒ‘", "å•¥æ„æ€", "???", "ä¸ç†è§£"],
    "description": "è¡¨ç¤ºå›°æƒ‘ä¸è§£",
    "emotion": "å›°æƒ‘",
    "safety_level": "safe"
  }
]
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- âœ… 150-200ä¸ªè¡¨æƒ…åŒ…
- âœ… æ¯ä¸ªè‡³å°‘3ä¸ªå…³é”®è¯
- âœ… JSONæ ¼å¼æ­£ç¡®

#### 2. æŸ¥è¯¢æ•°æ®é›† - `data/queries_test.json`

**é¢„æœŸäº¤ä»˜æ—¶é—´**ï¼šDay 3-4  
**æ ¼å¼**ï¼šJSONå¯¹è±¡

```json
{
  "train": [...],
  "test": [...],
  "ground_truth": {
    "train": {...},
    "test": {...}
  }
}
```

**éªŒæ”¶æ ‡å‡†**ï¼š
- âœ… æµ‹è¯•é›†100-160æ¡
- âœ… åŒ…å«ground_truthæ ‡æ³¨
- âœ… ä¸è®­ç»ƒé›†æ— é‡å 

#### æ¥æ”¶æµç¨‹

```bash
# 1. æˆå‘˜2é€šçŸ¥ï¼š"æ•°æ®å·²å°±ç»ª"
# 2. æ£€æŸ¥æ–‡ä»¶
ls -lh data/memes.json
ls -lh data/queries_test.json

# 3. å¿«é€ŸéªŒè¯
python -c "
import json
with open('data/memes.json') as f:
    memes = json.load(f)
print(f'è¡¨æƒ…åŒ…æ•°é‡: {len(memes)}')
"

# 4. åé¦ˆæˆå‘˜2
#    - å¦‚æœæœ‰é—®é¢˜ï¼šç«‹å³åé¦ˆä¿®æ­£
#    - å¦‚æœæ­£å¸¸ï¼šç¡®è®¤æ”¶åˆ°ï¼Œå¼€å§‹è¯„ä¼°
```

---

## ğŸ“¤ è¾“å‡ºäº¤ä»˜ï¼ˆç»™å…¶ä»–æˆå‘˜ï¼‰

### ç»™æˆå‘˜3ï¼ˆæ£€ç´¢ç³»ç»Ÿå·¥ç¨‹å¸ˆï¼‰

#### 1. è¡¨æƒ…åŒ…å‘é‡æ–‡ä»¶

**äº¤ä»˜æ—¶é—´**ï¼šDay 5-6ï¼ˆæ•°æ®å°±ç»ªå1-2å¤©ï¼‰  
**æ–‡ä»¶æ¸…å•**ï¼š

```
outputs/
â”œâ”€â”€ meme_embeddings.npy    # å‘é‡æ•°æ®
â”œâ”€â”€ meme_ids.json          # IDæ˜ å°„
â”œâ”€â”€ meme_texts.txt         # æ–‡æœ¬åˆ—è¡¨ï¼ˆè°ƒè¯•ç”¨ï¼‰
â””â”€â”€ metadata.json          # å…ƒæ•°æ®
```

**ä½¿ç”¨è¯´æ˜**ï¼š

```python
# æˆå‘˜3ä½¿ç”¨ç¤ºä¾‹
import numpy as np
import json

# åŠ è½½å‘é‡
embeddings = np.load('outputs/meme_embeddings.npy')  # shape: (N, D)
with open('outputs/meme_ids.json') as f:
    meme_ids = json.load(f)

# æ£€æŸ¥
print(f"å‘é‡æ•°é‡: {len(embeddings)}")
print(f"å‘é‡ç»´åº¦: {embeddings.shape[1]}")
print(f"IDæ•°é‡: {len(meme_ids)}")
assert len(embeddings) == len(meme_ids), "æ•°é‡ä¸åŒ¹é…ï¼"
```

#### 2. æ¨¡å‹ä¿¡æ¯

**æä¾›ç»™æˆå‘˜3çš„å…ƒæ•°æ®**ï¼š

| å­—æ®µ | å€¼ | è¯´æ˜ |
|------|---|------|
| æ¨¡å‹åç§° | `paraphrase-multilingual-MiniLM-L12-v2` | æˆ–å…¶ä»–é€‰å®šæ¨¡å‹ |
| å‘é‡ç»´åº¦ | 384 / 512 / 768 | å–å†³äºæ¨¡å‹ |
| æ•°æ®ç±»å‹ | float32 | NumPyæ•°ç»„ç±»å‹ |
| å½’ä¸€åŒ– | å¦ | å‘é‡æœªå½’ä¸€åŒ– |
| ç›¸ä¼¼åº¦è®¡ç®— | cosine_similarity | æ¨èä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ |

#### äº¤ä»˜æµç¨‹

```bash
# 1. æ‰“åŒ…è¾“å‡ºæ–‡ä»¶
cd outputs
tar -czf member4_outputs.tar.gz *.npy *.json *.txt

# 2. é€šçŸ¥æˆå‘˜3
#    ä¸»é¢˜ï¼šã€æˆå‘˜4â†’æˆå‘˜3ã€‘å‘é‡æ–‡ä»¶å·²å°±ç»ª
#    å†…å®¹ï¼š
#      - æ–‡ä»¶ä½ç½®ï¼šoutputs/member4_outputs.tar.gz
#      - å‘é‡ç»´åº¦ï¼š384
#      - æ¨¡å‹åç§°ï¼šparaphrase-multilingual-MiniLM-L12-v2
#      - ä½¿ç”¨è¯´æ˜ï¼šè§ outputs/metadata.json

# 3. ååŠ©é›†æˆæµ‹è¯•
#    å¦‚æˆå‘˜3æœ‰é—®é¢˜ï¼ŒååŠ©è°ƒè¯•
```

---

### ç»™æˆå‘˜1ï¼ˆé¡¹ç›®è´Ÿè´£äººï¼‰

#### 1. æ¨¡å‹è¯„ä¼°ç»“æœ

**äº¤ä»˜æ—¶é—´**ï¼šDay 6  
**æ–‡ä»¶æ¸…å•**ï¼š

```
outputs/
â”œâ”€â”€ model_comparison.csv      # æ¨¡å‹å¯¹æ¯”è¡¨
â””â”€â”€ evaluation_report.txt     # è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
```

#### 2. æ¨¡å‹é€‰å‹æŠ¥å‘Šï¼ˆ1é¡µï¼‰

**å†…å®¹ç»“æ„**ï¼š

```markdown
# æ¨¡å‹é€‰å‹æŠ¥å‘Š

## 1. è¯„ä¼°æ¦‚è¿°
- æµ‹è¯•æ¨¡å‹æ•°ï¼š4ä¸ª
- æµ‹è¯•æ•°æ®ï¼šXXXæ¡æŸ¥è¯¢ x XXXä¸ªè¡¨æƒ…åŒ…
- è¯„ä¼°æŒ‡æ ‡ï¼šRecall@1/3/5, MRR

## 2. æ¨¡å‹å¯¹æ¯”
| æ¨¡å‹ | Recall@3 | MRR | é€Ÿåº¦ |
|------|----------|-----|------|
| ... | ... | ... | ... |

## 3. æ¨èæ¨¡å‹
- é€‰æ‹©ï¼šXXX
- ç†ç”±ï¼šæ€§èƒ½ä¸é€Ÿåº¦å¹³è¡¡

## 4. é—®é¢˜ä¸æ”¹è¿›
- å½“å‰å±€é™ï¼šå°æ•°æ®é›†
- æ”¹è¿›æ–¹å‘ï¼šå¾®è°ƒã€æ•°æ®å¢å¼º
```

---

### ç»™æˆå‘˜5ï¼ˆå…¨æ ˆä¸éƒ¨ç½²å·¥ç¨‹å¸ˆï¼‰

#### 1. æ¨ç†æ¥å£

**æä¾›å¿«é€Ÿæ¨ç†è„šæœ¬**ï¼š

```python
# inference_api.py
from sentence_transformers import SentenceTransformer
import numpy as np
import json

class MemeRecommender:
    def __init__(self, model_path, embeddings_path, ids_path):
        """
        åˆå§‹åŒ–æ¨èå™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            embeddings_path: å‘é‡æ–‡ä»¶è·¯å¾„
            ids_path: IDæ˜ å°„æ–‡ä»¶è·¯å¾„
        """
        self.model = SentenceTransformer(model_path)
        self.meme_embeddings = np.load(embeddings_path)
        with open(ids_path) as f:
            self.meme_ids = json.load(f)
    
    def recommend(self, query_text: str, k: int = 3):
        """
        æ¨èTop-kè¡¨æƒ…åŒ…
        
        Args:
            query_text: ç”¨æˆ·è¾“å…¥çš„èŠå¤©å¥å­
            k: è¿”å›Top-kä¸ªç»“æœ
            
        Returns:
            [(meme_id, score), ...]
        """
        # ç¼–ç æŸ¥è¯¢
        query_emb = self.model.encode([query_text])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity([query_emb], self.meme_embeddings)[0]
        
        # è·å–Top-k
        top_k_indices = np.argsort(similarities)[-k:][::-1]
        results = [(self.meme_ids[i], float(similarities[i])) 
                   for i in top_k_indices]
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
recommender = MemeRecommender(
    model_path='paraphrase-multilingual-MiniLM-L12-v2',
    embeddings_path='outputs/meme_embeddings.npy',
    ids_path='outputs/meme_ids.json'
)

results = recommender.recommend("å“ˆå“ˆå“ˆå¤ªå¥½ç¬‘äº†", k=3)
print(results)
```

---

## ğŸ“ æ²Ÿé€šåè®®

### æ—¥å¸¸æ²Ÿé€š

| åœºæ™¯ | è”ç³»å¯¹è±¡ | æ–¹å¼ | é¢„æœŸå“åº”æ—¶é—´ |
|------|---------|------|------------|
| æ•°æ®æ ¼å¼é—®é¢˜ | æˆå‘˜2 | æ¶ˆæ¯/é‚®ä»¶ | 4å°æ—¶å†… |
| å‘é‡é›†æˆé—®é¢˜ | æˆå‘˜3 | æ¶ˆæ¯/é‚®ä»¶ | å½“å¤©å†… |
| è¯„ä¼°ç»“æœæ±‡æŠ¥ | æˆå‘˜1 | é‚®ä»¶ | 48å°æ—¶å†… |
| APIæ¥å£é—®é¢˜ | æˆå‘˜5 | æ¶ˆæ¯/è§†é¢‘ | å½“å¤©å†… |

### å…³é”®èŠ‚ç‚¹åŒæ­¥

#### Checkpoint 1: æ•°æ®éªŒæ”¶ï¼ˆDay 4ï¼‰
- **ä¸æˆå‘˜2**ï¼šç¡®è®¤æ•°æ®æ ¼å¼ã€è´¨é‡
- **ç»“æœ**ï¼šé€šè¿‡/éœ€ä¿®æ­£

#### Checkpoint 2: æ¨¡å‹é€‰å‹ï¼ˆDay 6ï¼‰
- **ä¸æˆå‘˜1**ï¼šæ±‡æŠ¥è¯„ä¼°ç»“æœ
- **ä¸æˆå‘˜3**ï¼šç¡®è®¤å‘é‡äº¤ä»˜æ—¶é—´

#### Checkpoint 3: é›†æˆæµ‹è¯•ï¼ˆDay 8ï¼‰
- **ä¸æˆå‘˜3**ï¼šååŠ©æ£€ç´¢ç³»ç»Ÿé›†æˆæµ‹è¯•
- **ä¸æˆå‘˜5**ï¼šç¡®è®¤æ¨ç†APIæ­£å¸¸

---

## ğŸ”§ è°ƒè¯•åä½œ

### é—®é¢˜1ï¼šå‘é‡ç»´åº¦ä¸åŒ¹é…

**ç°è±¡**ï¼šæˆå‘˜3æŠ¥å‘Šç»´åº¦é”™è¯¯  
**æ’æŸ¥**ï¼š
```python
# æˆå‘˜4æ£€æŸ¥
embeddings = np.load('outputs/meme_embeddings.npy')
print(embeddings.shape)  # åº”è¯¥æ˜¯ (N, D)

# æˆå‘˜3æ£€æŸ¥
model = SentenceTransformer('model_name')
print(model.get_sentence_embedding_dimension())
```

### é—®é¢˜2ï¼šæ£€ç´¢æ•ˆæœå·®

**ç°è±¡**ï¼šæˆå‘˜3æˆ–æˆå‘˜5åé¦ˆæ¨èç»“æœä¸å‡†  
**æ’æŸ¥æµç¨‹**ï¼š
1. ç¡®è®¤ä½¿ç”¨çš„æ¨¡å‹æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
3. æä¾›è°ƒè¯•è„šæœ¬ç»™æˆå‘˜3

```python
# è°ƒè¯•è„šæœ¬
query = "æµ‹è¯•å¥å­"
query_emb = model.encode([query])[0]
similarities = cosine_similarity([query_emb], meme_embeddings)[0]
top_5 = np.argsort(similarities)[-5:][::-1]
for idx in top_5:
    print(f"{meme_ids[idx]}: {similarities[idx]:.4f}")
```

---

## ğŸ“‹ è´¨é‡æ£€æŸ¥æ¸…å•

### äº¤ä»˜å‰è‡ªæ£€

**å‘é‡æ–‡ä»¶**ï¼š
- [ ] æ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»å–
- [ ] ç»´åº¦æ­£ç¡® (N, D)
- [ ] æ•°æ®ç±»å‹ float32
- [ ] æ— NaNæˆ–Infå€¼
- [ ] IDæ•°é‡ä¸å‘é‡æ•°é‡ä¸€è‡´

**è¯„ä¼°ç»“æœ**ï¼š
- [ ] æ‰€æœ‰æ¨¡å‹éƒ½æˆåŠŸè¯„ä¼°
- [ ] ç»“æœä¿å­˜ä¸ºCSV
- [ ] æŠ¥å‘ŠåŒ…å«æ¶ˆèåˆ†æ
- [ ] æ¨èæ¨¡å‹æœ‰æ˜ç¡®ç†ç”±

**æ–‡æ¡£**ï¼š
- [ ] metadata.json å®Œæ•´
- [ ] ä½¿ç”¨ç¤ºä¾‹æ­£ç¡®
- [ ] è”ç³»æ–¹å¼ç•™å­˜

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### æœ€ä½äº¤ä»˜æ ‡å‡†ï¼ˆå¿…é¡»ï¼‰
- âœ… è‡³å°‘è¯„ä¼°3ä¸ªæ¨¡å‹
- âœ… Recall@3 â‰¥ 0.55
- âœ… å‘é‡æ–‡ä»¶æ ¼å¼æ­£ç¡®
- âœ… æ¨ç†é€Ÿåº¦ < 300ms/queryï¼ˆCPUï¼‰

### ä¼˜ç§€æ ‡å‡†ï¼ˆåŠ åˆ†ï¼‰
- ğŸŒŸ è¯„ä¼°5+ä¸ªæ¨¡å‹
- ğŸŒŸ Recall@3 â‰¥ 0.65
- ğŸŒŸ  æä¾›å¾®è°ƒç‰ˆæœ¬å¯¹æ¯”
- ğŸŒŸ è¯¦ç»†çš„è¯¯å·®åˆ†æ

---

**è®°ä½ï¼šè‰¯å¥½çš„åä½œæ˜¯é¡¹ç›®æˆåŠŸçš„å…³é”®ï¼** ğŸ¤


