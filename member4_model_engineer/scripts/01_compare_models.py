#!/usr/bin/env python
"""
è„šæœ¬1ï¼šæ¨¡å‹å¯¹æ¯”å®éªŒ

åŠŸèƒ½ï¼š
- åŠ è½½é…ç½®æ–‡ä»¶ä¸­çš„å¤šä¸ªå€™é€‰æ¨¡å‹
- åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å„æ¨¡å‹æ€§èƒ½
- è¾“å‡ºå¯¹æ¯”ç»“æœï¼ˆCSV + æ§åˆ¶å°æŠ¥å‘Šï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/01_compare_models.py
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import time
from src.model_evaluator import MemeModelEvaluator
from src.utils import load_config, load_data, save_results, format_report


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸš€ MemeMatch æ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("="*70)
    
    # 1. åŠ è½½é…ç½®
    print("\nğŸ“– åŠ è½½é…ç½®...")
    config = load_config(str(project_root / "config/models.yaml"))
    models_config = config['models']
    print(f"   å¾…è¯„ä¼°æ¨¡å‹æ•°é‡: {len(models_config)}")
    
    # 2. åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    try:
        # ä¼˜å…ˆå°è¯•åŠ è½½å®Œæ•´æ•°æ®
        try:
            meme_ids, meme_texts, query_ids, query_texts, ground_truth = load_data(
                data_dir=str(project_root / "data"),
                meme_file="memes.json",
                query_file="queries_test.json",
                split="test"
            )
            print("   âœ… ä½¿ç”¨å®Œæ•´æµ‹è¯•æ•°æ®")
        except FileNotFoundError:
            # å¦‚æœå®Œæ•´æ•°æ®ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®
            print("   âš ï¸  å®Œæ•´æ•°æ®æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            meme_ids, meme_texts, query_ids, query_texts, ground_truth = load_data(
                data_dir=str(project_root / "data"),
                meme_file="memes_sample.json",
                query_file="queries_sample.json",
                split="test"
            )
            print("   â„¹ï¸  è¿™ä»…ç”¨äºæµ‹è¯•ï¼Œç­‰å¾…æˆå‘˜2æä¾›å®Œæ•´æ•°æ®")
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. data/ ç›®å½•æ˜¯å¦å­˜åœ¨")
        print("   2. æ˜¯å¦æœ‰ memes_sample.json å’Œ queries_sample.json")
        print("   3. æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆå‚è€ƒ data/README.mdï¼‰")
        return
    
    # 3. é€ä¸ªè¯„ä¼°æ¨¡å‹
    results = []
    all_reports = []
    
    for idx, model_config in enumerate(models_config, 1):
        model_name = model_config['name']
        
        print(f"\n{'='*70}")
        print(f"ğŸ“‹ [{idx}/{len(models_config)}] è¯„ä¼°æ¨¡å‹: {model_name}")
        print(f"   æè¿°: {model_config.get('description', 'N/A')}")
        print(f"   ç»´åº¦: {model_config.get('dimensions', 'N/A')}")
        print(f"{'='*70}")
        
        try:
            # åˆå§‹åŒ–è¯„ä¼°å™¨
            evaluator = MemeModelEvaluator(model_name)
            
            # è¯„ä¼°
            start_time = time.time()
            metrics, _, _, inference_time = evaluator.evaluate(
                query_texts=query_texts,
                meme_texts=meme_texts,
                ground_truth=ground_truth,
                query_ids=query_ids,
                meme_ids=meme_ids
            )
            total_time = time.time() - start_time
            
            # ä¿å­˜ç»“æœ
            result = {
                'model_name': model_name,
                'dimensions': model_config.get('dimensions', 'N/A'),
                'language': model_config.get('language', 'N/A'),
                'recall@1': metrics['recall@1'],
                'recall@3': metrics['recall@3'],
                'recall@5': metrics['recall@5'],
                'mrr': metrics['mrr'],
                'inference_time_s': inference_time,
                'total_time_s': total_time,
                'queries_per_sec': len(query_texts) / inference_time if inference_time > 0 else 0
            }
            results.append(result)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = format_report(model_name, metrics, inference_time)
            all_reports.append(report)
            print(report)
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            print(f"   è·³è¿‡æ­¤æ¨¡å‹ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...")
            continue
    
    # 4. è¾“å‡ºæ±‡æ€»ç»“æœ
    if not results:
        print("\nâŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹ï¼")
        return
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š è¯„ä¼°å®Œæˆï¼å…±æˆåŠŸè¯„ä¼° {len(results)} ä¸ªæ¨¡å‹")
    print(f"{'='*70}")
    
    # 5. ä¿å­˜ç»“æœ
    output_dir = project_root / "outputs"
    output_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜CSV
    csv_path = output_dir / "model_comparison.csv"
    save_results(results, str(csv_path))
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_path = output_dir / "evaluation_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("MemeMatch æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
        f.write("="*70 + "\n\n")
        f.write(f"æµ‹è¯•é›†å¤§å°: {len(query_texts)} æ¡æŸ¥è¯¢\n")
        f.write(f"è¡¨æƒ…åŒ…æ•°é‡: {len(meme_texts)} ä¸ª\n")
        f.write(f"è¯„ä¼°æ¨¡å‹æ•°: {len(results)} ä¸ª\n\n")
        for report in all_reports:
            f.write(report + "\n")
    print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # 6. æ¨èæœ€ä½³æ¨¡å‹
    best_by_recall3 = max(results, key=lambda x: x['recall@3'])
    best_by_speed = max(results, key=lambda x: x['queries_per_sec'])
    
    print(f"\n{'='*70}")
    print(f"ğŸ† æœ€ä½³æ¨¡å‹æ¨è")
    print(f"{'='*70}")
    print(f"ğŸ“ˆ å‡†ç¡®ç‡æœ€é«˜ (Recall@3): {best_by_recall3['model_name']}")
    print(f"   Recall@3 = {best_by_recall3['recall@3']:.4f}")
    print(f"\nâš¡ é€Ÿåº¦æœ€å¿«: {best_by_speed['model_name']}")
    print(f"   é€Ÿåº¦ = {best_by_speed['queries_per_sec']:.1f} å¥/ç§’")
    print(f"{'='*70}")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. æŸ¥çœ‹è¯¦ç»†ç»“æœ: {csv_path}")
    print(f"   2. é€‰æ‹©æœ€ä½³æ¨¡å‹è¿›è¡Œå‘é‡å¯¼å‡º")
    print(f"   3. è¿è¡Œ: python scripts/02_export_embeddings.py --model <æ¨¡å‹å>")
    print()


if __name__ == "__main__":
    main()


