import faiss
import numpy as np
import json
import os
import time
import torch
import cn_clip.clip as clip
from sentence_transformers import SentenceTransformer

try:
    from . import config
except ImportError:
    import config

class SearchEngine:
    def __init__(self):
        print("âš™ï¸ åˆå§‹åŒ– CN-CLIP å¼ºåŠ›æœç´¢å¼•æ“ (RRFèåˆç‰ˆ)...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.clip_model = None # CN-CLIP
        self.text_model = None # M3E
        self.image_index = None
        self.text_index = None
        self.metadata = []
        self.index_size = 0
        self._load_resources()

    def _load_resources(self):
        """åŠ è½½èµ„æºï¼šCN-CLIP + M3E + FAISS"""
        try:
            print(f"ğŸ”„ åŠ è½½ CN-CLIP æ¨¡å‹: {config.MODEL_ARCH} (Device: {self.device})...")
            # download_root='./' é˜²æ­¢é‡å¤ä¸‹è½½
            self.clip_model, _ = clip.load_from_name(config.MODEL_ARCH, device=self.device, download_root='./')
            self.clip_model.eval()
            
            print(f"ğŸ”„ åŠ è½½å†…å®¹æ¨¡å‹: {config.TEXT_MODEL_NAME}...")
            self.text_model = SentenceTransformer(config.TEXT_MODEL_NAME)
            
            print(f"ğŸ”„ åŠ è½½å›¾åƒç´¢å¼•: {config.IMAGE_FAISS_INDEX_FILE}...")
            self.image_index = faiss.read_index(config.IMAGE_FAISS_INDEX_FILE)
            
            print(f"ğŸ”„ åŠ è½½æ–‡æœ¬ç´¢å¼•: {config.TEXT_FAISS_INDEX_FILE}...")
            self.text_index = faiss.read_index(config.TEXT_FAISS_INDEX_FILE)
            
            print(f"ğŸ”„ åŠ è½½å…ƒæ•°æ®: {config.METADATA_FILE}...")
            with open(config.METADATA_FILE, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            
            self.index_size = len(self.metadata)
            print(f"âœ… æœç´¢å¼•æ“å‡†å¤‡å°±ç»ª (å…± {self.index_size} æ¡æ•°æ®)")
            
        except Exception as e:
            print(f"âŒ åŠ è½½èµ„æºå¤±è´¥: {e}")
            self.image_index = None

    def _get_ranks(self, I_result):
        """å°† FAISS ç´¢å¼•çŸ©é˜µè½¬æ¢ä¸º {id: rank} å­—å…¸"""
        # I_result[0] æ˜¯ Top-K çš„ ID åˆ—è¡¨
        return {id_val: rank for rank, id_val in enumerate(I_result[0]) if id_val != -1}

    def search_meme_internal(self, query: str, top_k: int, min_score: float) -> dict:
        # ğŸ” [Log] æ‰“å°è¾“å…¥å‚æ•° (ä¿ç•™ä½ è¦æ±‚çš„è¯¦ç»†æ—¥å¿—)
        print(f"\n{'='*60}")
        print(f"ğŸ” [search_meme_internal] è¾“å…¥å‚æ•°:")
        print(f"   query: '{query}'")
        print(f"   top_k: {top_k}")
        print(f"   min_score: {min_score}")
        print(f"{'='*60}\n")

        start_time = time.time()
        
        if not self.image_index or not self.text_index:
            raise Exception("FAISS index not loaded")

        # --- 1. å®šä¹‰å‚æ•° ---
        SEARCH_K = max(100, top_k * 10)
        K_CONST = 60
        CONTENT_WEIGHT = 0.2

        # --- 2. CN-CLIP å›¾åƒè·¯æœç´¢ (ä¸­æ–‡ Query -> CN-CLIP -> Image Features) ---
        text = clip.tokenize([query]).to(self.device)
        with torch.no_grad():
            query_features = self.clip_model.encode_text(text)
            query_features /= query_features.norm(dim=-1, keepdim=True)
        
        query_vector_image = query_features.cpu().numpy().astype('float32')
        D_img, I_img = self.image_index.search(query_vector_image, SEARCH_K)

        # --- 3. M3E æ–‡æœ¬è·¯æœç´¢ (ä¸­æ–‡ Query -> M3E -> Content Features) ---
        query_vector_text = self.text_model.encode([query])
        faiss.normalize_L2(query_vector_text)
        D_txt, I_txt = self.text_index.search(query_vector_text, SEARCH_K)

        # --- 4. å‡†å¤‡ Rank æ•°æ® ---
        image_ranks = self._get_ranks(I_img)
        text_ranks = self._get_ranks(I_txt)
        all_ids = set(image_ranks.keys()) | set(text_ranks.keys())

        # --- 5. RRF èåˆè®¡ç®— ---
        # è®¡ç®—ç†è®ºæœ€å¤§åˆ† (åˆ†æ¯)
        max_image_score_part = (1.0 * (1.0 / (K_CONST + 0)))
        max_content_score_part = (CONTENT_WEIGHT * (1.0 / (K_CONST + 0)))
        
        fused_normalized_scores = {}
        
        # è°ƒè¯•ï¼šæ‰“å° Top 1 çš„æ’åæƒ…å†µ
        if I_img[0][0] != -1:
            top_img_id = I_img[0][0]
            print(f"ğŸ” [Debug] å›¾åƒè·¯ç¬¬1å ID: {top_img_id} (Rank 0)")
        else:
            print(f"ğŸ” [Debug] å›¾åƒè·¯æœªæ‰¾åˆ°ç»“æœ")

        for id_val in all_ids:
            if id_val >= len(self.metadata): continue
            meta = self.metadata[id_val]
            
            rrf_score = 0.0
            # åŸºç¡€åˆ†æ¯è‡³å°‘åŒ…å«å›¾åƒéƒ¨åˆ†
            max_possible = max_image_score_part 
            
            # --- å›¾åƒè·¯å¾—åˆ† ---
            rank = image_ranks.get(id_val)
            if rank is not None:
                rrf_score += 1.0 * (1.0 / (K_CONST + rank))
            
            # --- æ–‡æœ¬è·¯å¾—åˆ† ---
            if meta.get('content'):
                max_possible += max_content_score_part
                rank = text_ranks.get(id_val)
                if rank is not None:
                    rrf_score += CONTENT_WEIGHT * (1.0 / (K_CONST + rank))
            
            # å½’ä¸€åŒ–
            normalized = (rrf_score / max_possible) if max_possible > 0 else 0.0
            fused_normalized_scores[id_val] = min(normalized, 1.0)
        
        # æ’åº
        sorted_results = sorted(fused_normalized_scores.items(), key=lambda item: item[1], reverse=True)
        
        # [Log] æ‰“å°æ’ååˆ†æ
        if sorted_results:
            top_id, top_score = sorted_results[0]
            img_rank = image_ranks.get(top_id, "æ²¡è¿›å‰100")
            txt_rank = text_ranks.get(top_id, "æ²¡è¿›å‰100")
            print(f"ğŸ§ [åˆ†æ] æœ€ç»ˆç¬¬1å (ID: {top_id}) å¾—åˆ†: {top_score:.4f}")
            print(f"    - å›¾åƒæ’å: {img_rank} (å¦‚æœæ˜¯60å·¦å³ï¼Œåˆ†æ•°å°±æ˜¯0.4)")
            print(f"    - æ–‡æœ¬æ’å: {txt_rank}")

        # --- 6. è¿‡æ»¤ & ç»„è£… (æŒ‰ç…§ä½ è¦æ±‚çš„æŒ‡å®šç»“æ„) ---
        final_candidates = []
        filtered_count = 0
        
        for id_val, score in sorted_results:
            # é˜ˆå€¼è¿‡æ»¤
            if score < min_score:
                filtered_count += 1
                continue
            
            # æ”¶é›† Top-K
            if len(final_candidates) < top_k:
                meta = self.metadata[id_val]
                final_candidates.append({
                    "image_path": os.path.join(config.IMAGES_DIR, meta['filename']),
                    "score": round(score, 4),
                    "tags": [meta['emotion']] if meta.get('emotion') else [],
                    "metadata": {
                        "file_size": meta.get('file_size', 0),
                        "dimensions": meta.get('dimensions', [0,0]),
                        "format": meta.get('format', 'unknown')
                    }
                })

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "success": True,
            "data": {
                "query": query,
                "results": final_candidates,
                "total": len(final_candidates),
                "filtered": filtered_count
            },
            "metadata": {
                "search_time": time.time() - start_time,
                "index_size": self.index_size
            }
        }

        # ğŸ“¤ [Log] æ‰“å°è¾“å‡ºç»“æœ (ä¿ç•™ä½ è¦æ±‚çš„è¯¦ç»†æ—¥å¿—)
        print(f"\n{'='*60}")
        print(f"ğŸ“¤ [search_meme_internal] è¾“å‡ºç»“æœ:")
        print(f"   success: {result['success']}")
        print(f"   total_results: {result['data']['total']}")
        print(f"   search_time: {result['metadata']['search_time']:.4f}s")
        if result['data']['results']:
            print(f"   Top-1:")
            top1 = result['data']['results'][0]
            print(f"      - path: {top1['image_path']}")
            print(f"      - score: {top1['score']}")
            print(f"      - tags: {top1['tags']}")
        print(f"{'='*60}\n")

        return result

# --- å•ä¾‹ä¸æ¥å£ ---
_engine_instance = None
def get_search_engine():
    global _engine_instance
    if _engine_instance is None:
        _engine_instance = SearchEngine()
    return _engine_instance

def search_meme(query: str, top_k: int = 5, min_score: float = 0.0) -> dict:
    return get_search_engine().search_meme_internal(query, top_k, min_score)