import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import cn_clip.clip as clip
import torch
from PIL import Image
import os
import json
from . import config

def build_embeddings():
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âš™ï¸ [Builder] åŠ è½½ CN-CLIP æ¨¡å‹: {config.MODEL_ARCH} (Device: {device})...")
    
    # åŠ è½½ CN-CLIP
    model, preprocess = clip.load_from_name(config.MODEL_ARCH, device=device, download_root='./')
    model.eval()
    
    print(f"ğŸ”„ [Builder] åŠ è½½æ–‡æœ¬æ¨¡å‹: {config.TEXT_MODEL_NAME}...")
    text_model = SentenceTransformer(config.TEXT_MODEL_NAME)
    
    print(f"ğŸ“‚ è¯»å– CSV: {config.CSV_PATH}")
    try:
        df = pd.read_csv(config.CSV_PATH).fillna({'content': '', 'emotion': ''})
    except Exception as e:
        print(f"âŒ CSV è¯»å–å¤±è´¥: {e}")
        return

    valid_image_embeddings = []
    valid_text_embeddings = []
    valid_metadata = []
    
    print("ğŸš€ å¼€å§‹ç”Ÿæˆå‘é‡ (CN-CLIP)...")
    
    for index, row in df.iterrows():
        filename = str(row['filename'])
        content = str(row['content'])
        emotion = str(row['emotion'])
        img_path = os.path.join(config.IMAGES_DIR, filename)
        
        if not os.path.exists(img_path):
            continue
            
        try:
            # CN-CLIP å›¾åƒé¢„å¤„ç†
            image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)
            with torch.no_grad():
                image_features = model.encode_image(image)
                image_features /= image_features.norm(dim=-1, keepdim=True) # å½’ä¸€åŒ–
                
            image_emb = image_features.cpu().numpy()[0]
            text_emb = text_model.encode(content)
            
            valid_image_embeddings.append(image_emb)
            valid_text_embeddings.append(text_emb)
            
            valid_metadata.append({
                "id": len(valid_metadata),
                "filename": filename,
                "content": content,
                "emotion": emotion,
                "file_size": os.path.getsize(img_path)
            })
            
            if len(valid_metadata) % 100 == 0:
                print(f"âœ… å·²å¤„ç† {len(valid_metadata)} å¼ ")
                
        except Exception as e:
            print(f"âŒ é”™è¯¯ {filename}: {e}")

    if valid_metadata:
        np.save(config.IMAGE_EMBEDDING_FILE, np.array(valid_image_embeddings).astype('float32'))
        np.save(config.TEXT_EMBEDDING_FILE, np.array(valid_text_embeddings).astype('float32'))
        with open(config.METADATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(valid_metadata, f, ensure_ascii=False, indent=2)
        print("ğŸ’¾ å‘é‡ç”Ÿæˆå®Œæ¯•ï¼")
    else:
        print("âŒ æœªç”Ÿæˆä»»ä½•æ•°æ®")

if __name__ == "__main__":
    build_embeddings()